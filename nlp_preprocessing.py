# -*- coding: utf-8 -*-
"""NLP2_final_version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Z1QyY-SnnU_heT0ikpgHjNlN8AQhkk9
"""

!pip install transformers

from pips_fr import pipeline_fr
from pipelines import pipeline
from nltk import wordpunct_tokenize
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')
nltk.download('punkt')

nlp_en = pipeline("question-generation")
nlp_fr = pipeline_fr("multitask-qa-qg")

class Question_Answering :
    
  def getQuestion(text):
    texteDecouper = Question_Answering.decouperTexte(text)
    langue = Language.getLanguage(texteDecouper[0])
    print(langue)
    nlp = Question_Answering.language(langue)
    _liste_question = []
    for texte in texteDecouper:
      result = nlp(texte)
      for reponse in result :
        _liste_question.append(reponse)
    return _liste_question
  
  def language(langue):
    if(langue == "english") :
      return nlp_en
    if(langue == "french") :
      return nlp_fr

  def decouperTexte(txt):
    #Transformation du texte en liste de phrases
    tokenized_sentence = sent_tokenize(txt)
    #Taille actuelle de one_input
    compteur = 0
    #résultat final
    all_inputs = []
    #Une entrée du transformers, i.e une ou plusieurs pharses d'au total 511 caractères ou moins
    one_input = ""
    #Pour chaque phrase
    for sentence in tokenized_sentence :
      #On récupère la taille de la phrase
      sentence_size = len(sentence)
      #Si la phrase à elle seule fait plus de 512 caractères
      if sentence_size >511:
        #Pour conserver l'ordre du texte
        #Si notre entrée actuelle n'est pas vide, on l'ajoute à la liste des entrée
        if one_input != "":
          all_inputs.append(one_input)
          #La nouvelle taille de la nouvelle entrée est donc de 0
          compteur = 0
          #Notre nouvelle entrée est vide
          one_input = ""
        #Notre phrase trop longue est tronquée
        sentence = sentence[0:511]
        #Puis ajoutée à la liste de toutes les entrées
        all_inputs.append(sentence)
      #Sinon
      else :
        #On regarde si la taille de l'entrée établie jusque là + celle de notre nouvelle phrase est inférieure à 512
        if compteur + sentence_size < 511:
          #Si oui la nouvelle dimension de notre entrée est déterminée
          compteur += sentence_size
          #On ajpoute la phrase à notre entrée
          one_input += sentence
        #Sinon
        else:
          #Si on ajoute la phrase à notre entrée, on dépasse les 512 caractères
          #On ajoute donc notre entrée actuelle à la liste de toutes les entrées
          all_inputs.append(one_input)
          #Notre nouvelle entrée actuelle devient la phrase sentence
          one_input = sentence
          #La nouvelle taille est la taile de la phrase sentence
          compteur = sentence_size
    #Il faut ajouter à la fin la dernière entrée si elle existe
    if one_input != "":
      all_inputs.append(one_input)
    return all_inputs

class Language :
  def _calc_ratios(text):
    _ratios = {}
    tokens = wordpunct_tokenize(text)
    words = [word.lower() for word in tokens]
    for lang in stopwords.fileids():
      stopwords_set = set(stopwords.words(lang))
      words_set = set(words)
      common_words = words_set.intersection(stopwords_set)
      ratios[lang] = len(common_words)

      return ratios

  def _detect_language(text):
    ratios = _calc_ratios(text)
    most_rated_language = max(ratios, key=ratios.get)
    return most_rated_language

  def getLanguage(text):
    return str(detect_language(text))
